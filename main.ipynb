{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/newbee/Projects/ai/chem_pred_transformer/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import ChemBL35Dataset\n",
    "from tokenizer import SMILESTokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "smiles_file = \"chembl_35.smi\"\n",
    "tokenizer_dir = \"trained_tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SMILESTokenizer.from_pretrained(tokenizer_dir)\n",
    "if tokenizer.mask_token is None:\n",
    "\ttokenizer.add_special_tokens({\"mask_token\": \"<mask>\"})\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "ds = ChemBL35Dataset(smiles_file, tokenizer, max_length=256, noise_prob=0.15)\n",
    "train_size = int(0.9 * len(ds))\n",
    "val_size = len(ds) - train_size\n",
    "train_ds, val_ds = random_split(ds, [train_size, val_size])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=10)\n",
    "val_dl = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from einops import rearrange, repeat\n",
    "import xformers.ops as xops\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from models.utils import DyT, FeedForward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking some number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, tensor(8.3180))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, torch.log1p(torch.tensor(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch = next(iter(train_dl))\n",
    "first_batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "\tfreqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "\tt = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "\tfreqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "\tfreqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "\treturn freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "\tndim = x.ndim\n",
    "\n",
    "\tassert freqs_cis.shape == (x.shape[1], x.shape[-1]), (\n",
    "\t\tf\"freqs_cis shape {freqs_cis.shape} needs to be {(x.shape[1], x.shape[-1])}\"\n",
    "\t)\n",
    "\n",
    "\tshape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "\n",
    "\treturn freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "\txq: torch.Tensor,\n",
    "\txk: torch.Tensor,\n",
    "\tfreqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\txq_complex = torch.view_as_complex(\n",
    "\t\trearrange(xq.float(), \"... (n two) -> ... n two\", two=2)\n",
    "\t)\n",
    "\txk_complex = torch.view_as_complex(\n",
    "\t\trearrange(xk.float(), \"... (n two) -> ... n two\", two=2)\n",
    "\t)\n",
    "\n",
    "\tfreqs_cis = reshape_for_broadcast(freqs_cis, xq_complex)\n",
    "\n",
    "\txq_out = torch.view_as_real(xq_complex * freqs_cis).flatten(3)\n",
    "\txk_out = torch.view_as_real(xk_complex * freqs_cis).flatten(3)\n",
    "\n",
    "\treturn xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, d_model, n_heads, max_seq_len, dropout=0.1):\n",
    "\t\tsuper(MultiHeadAttention, self).__init__()\n",
    "\t\tassert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\t\t\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.n_heads = n_heads\n",
    "\t\tself.d_k = d_model // n_heads\n",
    "\t\t\n",
    "\t\tself.q_proj = nn.Linear(d_model, d_model)\n",
    "\t\tself.k_proj = nn.Linear(d_model, d_model)\n",
    "\t\tself.v_proj = nn.Linear(d_model, d_model)\n",
    "\t\tself.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "\t\tself.p = dropout\n",
    "\t\t# self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\t\tnn.init.xavier_uniform_(self.q_proj.weight, gain=1 / (2 ** 0.5))\n",
    "\t\tnn.init.xavier_uniform_(self.k_proj.weight, gain=1 / (2 ** 0.5))\n",
    "\t\tnn.init.xavier_uniform_(self.v_proj.weight, gain=1 / (2 ** 0.5))\n",
    "\t\tnn.init.xavier_uniform_(self.out_proj.weight)\n",
    "\t\tnn.init.zeros_(self.out_proj.bias)\n",
    "\t\t\t\n",
    "\tdef scaled_dot_product_attention(\n",
    "\t\tself, \n",
    "  \tQ: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n",
    "\t\tmask: Optional[torch.Tensor] = None, is_causal: bool = False\n",
    "\t):\n",
    "\t\t# attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\t\t# if mask is not None:\n",
    "\t\t# \tattn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "\t\t# attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\t\t# output = torch.matmul(attn_probs, V)\n",
    "\t\t# output = self.dropout(output)\n",
    "\n",
    "\t\toutput = xops.memory_efficient_attention(\n",
    "\t\t\tQ, K, V, \n",
    "\t\t\tp=self.p,\n",
    "\t\t\tattn_bias=None if not is_causal else xops.LowerTriangularMask(),\n",
    "\t\t)\n",
    "\n",
    "\t\treturn output\n",
    "\t\t\t\n",
    "\tdef split_heads(self, x):\n",
    "\t\t# return rearrange(x, 'b s (h d) -> b h s d', h=self.n_heads)\n",
    "\t\treturn rearrange(x, 'b s (h d) -> b s h d', h=self.n_heads)\n",
    "\t\t\t\n",
    "\tdef combine_heads(self, x):\n",
    "\t\t# return rearrange(x, 'b h s d -> b s (h d)', h=self.n_heads)\n",
    "\t\treturn rearrange(x, 'b s h d -> b s (h d)', h=self.n_heads)\n",
    "\t\t\t\n",
    "\tdef forward(\n",
    "\t\tself, \n",
    "  \tQ: torch.Tensor, K: torch.Tensor, V: torch.Tensor, freqs_cis: torch.Tensor,\n",
    "\t\tmask: Optional[torch.Tensor] = None, is_causal: bool = False\n",
    "\t):\n",
    "\t\tQ = self.split_heads(self.q_proj(Q))\n",
    "\t\tK = self.split_heads(self.k_proj(K))\n",
    "\t\tV = self.split_heads(self.v_proj(V))\n",
    "\n",
    "\t\tQ, K = apply_rotary_emb(Q, K, freqs_cis=freqs_cis)\n",
    "\t\t\n",
    "\t\tattn_output = self.scaled_dot_product_attention(Q, K, V, mask, is_causal=is_causal)\n",
    "\t\toutput = self.out_proj(self.combine_heads(attn_output))\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself, d_model: int, n_heads: int, d_ff: int = 3072,\n",
    "\t\tdropout: float = 0.2, max_seq_len: int = 1024, use_layerscale: bool = True,\n",
    "\t\tnorm_layer=nn.LayerNorm,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.self_attn = MultiHeadAttention(d_model, n_heads, max_seq_len, dropout)\n",
    "\t\tself.self_attn_norm = norm_layer(d_model)\n",
    "\t\tself.self_attn_dropout = nn.Dropout(dropout)\n",
    "\t\tself.attn_layer_scale = nn.Parameter(torch.ones(d_model) * 1e-4) if use_layerscale else None\n",
    "\n",
    "\t\tself.ff_norm = norm_layer(d_model)\n",
    "\t\tself.ff = FeedForward(d_model, d_ff, dropout)\n",
    "\t\tself.ff_dropout = nn.Dropout(dropout)\n",
    "\t\tself.ff_layer_scale = nn.Parameter(torch.ones(d_model) * 1e-4) if use_layerscale else None\n",
    "\t\t\t\n",
    "\tdef forward(self, src: torch.Tensor, freqs_cis: torch.Tensor, src_mask: Optional[torch.Tensor] = None):\n",
    "\t\tnorm_src = self.self_attn_norm(src)\n",
    "\t\tattn_out = self.self_attn(norm_src, norm_src, norm_src, freqs_cis, src_mask)\n",
    "\t\tattn_out = self.self_attn_dropout(attn_out)\n",
    "\t\tif self.attn_layer_scale is not None:\n",
    "\t\t\tsrc = src + self.attn_layer_scale * attn_out\n",
    "\t\telse:\n",
    "\t\t\tsrc = src + attn_out\n",
    "\n",
    "\t\tnorm_src = self.ff_norm(src)\n",
    "\t\tff_out = self.ff(norm_src)\n",
    "\t\tff_out = self.ff_dropout(ff_out)\n",
    "\t\tif self.ff_layer_scale is not None:\n",
    "\t\t\tsrc = src + self.ff_layer_scale * ff_out\n",
    "\t\telse:\n",
    "\t\t\tsrc = src + ff_out\n",
    "\n",
    "\t\treturn src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself, d_model: int, n_heads: int, d_ff: int = 3072,\n",
    "\t\tdropout: float = 0.2, max_seq_len: int = 1024, use_layerscale: bool = True,\n",
    "\t\tnorm_layer=nn.LayerNorm,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.self_attn = MultiHeadAttention(d_model, n_heads, max_seq_len, dropout)\n",
    "\t\tself.self_attn_norm = norm_layer(d_model)\n",
    "\t\tself.self_attn_dropout = nn.Dropout(dropout)\n",
    "\t\tself.self_attn_layer_scale = nn.Parameter(torch.ones(d_model) * 1e-4) if use_layerscale else None\n",
    "\n",
    "\t\tself.cross_attn = MultiHeadAttention(d_model, n_heads, max_seq_len, dropout)\n",
    "\t\tself.cross_attn_norm = norm_layer(d_model)\n",
    "\t\tself.cross_attn_dropout = nn.Dropout(dropout)\n",
    "\t\tself.cross_attn_layer_scale = nn.Parameter(torch.ones(d_model) * 1e-4) if use_layerscale else None\n",
    "\n",
    "\t\tself.ff_norm = norm_layer(d_model)\n",
    "\t\tself.ff = FeedForward(d_model, d_ff, dropout)\n",
    "\t\tself.ff_dropout = nn.Dropout(dropout)\n",
    "\t\tself.ff_layer_scale = nn.Parameter(torch.ones(d_model) * 1e-4) if use_layerscale else None\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself, tgt: torch.Tensor, memory: torch.Tensor, freqs_cis: torch.Tensor,\n",
    "\t\ttgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,\n",
    "\t):\n",
    "\t\tnorm_tgt = self.self_attn_norm(tgt)\n",
    "\t\tself_attn_out = self.self_attn(norm_tgt, norm_tgt, norm_tgt, freqs_cis, tgt_mask, is_causal=True)\n",
    "\t\tself_attn_out = self.self_attn_dropout(self_attn_out)\n",
    "\t\tif self.self_attn_layer_scale is not None:\n",
    "\t\t\ttgt = tgt + self.self_attn_layer_scale * self_attn_out\n",
    "\t\telse:\n",
    "\t\t\ttgt = tgt + self_attn_out\n",
    "\n",
    "\t\tnorm_tgt = self.cross_attn_norm(tgt)\n",
    "\t\tcross_attn_out = self.cross_attn(norm_tgt, memory, memory, freqs_cis, memory_mask)\n",
    "\t\tcross_attn_out = self.cross_attn_dropout(cross_attn_out)\n",
    "\t\tif self.cross_attn_layer_scale is not None:\n",
    "\t\t\ttgt = tgt + self.cross_attn_layer_scale * cross_attn_out\n",
    "\t\telse:\n",
    "\t\t\ttgt = tgt + cross_attn_out\n",
    "\n",
    "\t\tnorm_tgt = self.ff_norm(tgt)\n",
    "\t\tff_out = self.ff(norm_tgt)\n",
    "\t\tff_out = self.ff_dropout(ff_out)\n",
    "\t\tif self.ff_layer_scale is not None:\n",
    "\t\t\ttgt = tgt + self.ff_layer_scale * ff_out\n",
    "\t\telse:\n",
    "\t\t\ttgt = tgt + ff_out\n",
    "\n",
    "\t\treturn tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tsrc_vocab_size, tgt_vocab_size, \n",
    "\t\td_model, n_heads, n_layers, d_ff, max_seq_len, dropout,\n",
    "\t\tnorm_layer=nn.LayerNorm,\n",
    "\t):\n",
    "\t\tsuper(Transformer, self).__init__()\n",
    "\t\tself.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "\t\tself.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "\t\tself.encoder_layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout, norm_layer=norm_layer) for _ in range(n_layers)])\n",
    "\t\tself.decoder_layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout, norm_layer=norm_layer) for _ in range(n_layers)])\n",
    "\n",
    "\t\tself.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\t\tself.freqs_cis = precompute_freqs_cis(d_model // n_heads, max_seq_len * 2)\n",
    "\n",
    "\tdef forward(self, src, tgt):\n",
    "\t\tsrc_embedded = self.dropout(self.encoder_embedding(src))\n",
    "\t\ttgt_embedded = self.dropout(self.decoder_embedding(tgt))\n",
    "\n",
    "\t\t_, seq_len = src.shape\n",
    "\n",
    "\t\tfreqs_cis = self.freqs_cis[:seq_len].to(src.device)\n",
    "\n",
    "\t\tenc_output = src_embedded\n",
    "\t\tfor enc_layer in self.encoder_layers:\n",
    "\t\t\tenc_output = enc_layer(enc_output, freqs_cis)\n",
    "\n",
    "\t\tdec_output = tgt_embedded\n",
    "\t\tfor dec_layer in self.decoder_layers:\n",
    "\t\t\tdec_output = dec_layer(dec_output, enc_output, freqs_cis)\n",
    "\n",
    "\t\toutput = self.fc(dec_output)\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BART(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself, vocab_size: int,\n",
    "\t\td_model: int = 768, n_heads: int = 12,\n",
    "\t\tn_enc_layers: int = 6, n_dec_layers: int = 6,\n",
    "\t\td_ff: int = 3072, max_seq_len: int = 1024,\n",
    "\t\tdropout: float = 0.2,\n",
    "\t\tnorm_layer=nn.LayerNorm,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.vocab_size = vocab_size\n",
    "\t\tself.d_model = d_model\n",
    "\n",
    "\t\tself.enc_emb = nn.Embedding(vocab_size, d_model)\n",
    "\t\tself.dec_emb = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "\t\tself.freqs_cis = precompute_freqs_cis(d_model // n_heads, max_seq_len * 2)\n",
    "\n",
    "\t\tself.enc_layers = nn.ModuleList([\n",
    "\t\t\tEncoderLayer(d_model, n_heads, d_ff, dropout, max_seq_len, norm_layer=norm_layer)\n",
    "\t\t\tfor _ in range(n_enc_layers)\n",
    "\t\t])\n",
    "\n",
    "\t\tself.dec_layers = nn.ModuleList([\n",
    "\t\t\tDecoderLayer(d_model, n_heads, d_ff, dropout, max_seq_len, norm_layer=norm_layer)\n",
    "\t\t\tfor _ in range(n_dec_layers)\n",
    "\t\t])\n",
    "\n",
    "\t\tself.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\tdef encode(self, src: torch.Tensor, freqs_cis: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "\t\t# x = self.enc_emb(src) * math.sqrt(self.d_model)\n",
    "\n",
    "\t\tx = self.enc_emb(src)\n",
    "\t\tfor layer in self.enc_layers:\n",
    "\t\t\tx = layer(x, freqs_cis, src_mask)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\tdef decode(\n",
    "\t\tself, tgt: torch.Tensor, memory: torch.Tensor,\n",
    "\t\tfreqs_cis: torch.Tensor,\n",
    "\t\ttgt_mask: Optional[torch.Tensor] = None,\n",
    "\t\tmemory_mask: Optional[torch.Tensor] = None,\n",
    "\t) -> torch.Tensor:\n",
    "\t\t# x = self.dec_emb(tgt) * math.sqrt(self.d_model)\n",
    "\n",
    "\t\tx = self.dec_emb(tgt)\n",
    "\t\tfor layer in self.dec_layers:\n",
    "\t\t\tx = layer(x, memory, freqs_cis, tgt_mask, memory_mask)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None):\n",
    "\t\t_, seq_len = src.shape\n",
    "\t\tfreqs_cis = self.freqs_cis[:seq_len].to(src.device)\n",
    "\n",
    "\t\tenc_out = self.encode(src, freqs_cis, src_mask)\n",
    "\t\tdec_out = self.decode(tgt, enc_out, freqs_cis, tgt_mask)\n",
    "\n",
    "\t\tout = self.fc_out(dec_out)\n",
    "\t\treturn self.dropout(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "src_vocab_size = vocab_size\n",
    "tgt_vocab_size = vocab_size\n",
    "d_model = 256\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "d_ff = 512 \n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "# transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, norm_layer=DyT)\n",
    "transformer = BART(vocab_size, d_model, num_heads, num_layers, num_layers, d_ff, max_seq_length, dropout, norm_layer=DyT)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 36.806392669677734\n",
      "Epoch: 2, Loss: 36.32545471191406\n",
      "Epoch: 3, Loss: 35.86148452758789\n",
      "Epoch: 4, Loss: 35.36692428588867\n",
      "Epoch: 5, Loss: 35.016841888427734\n",
      "Epoch: 6, Loss: 34.54901123046875\n",
      "Epoch: 7, Loss: 34.08950424194336\n",
      "Epoch: 8, Loss: 33.622737884521484\n",
      "Epoch: 9, Loss: 33.1419677734375\n",
      "Epoch: 10, Loss: 32.739952087402344\n",
      "Epoch: 11, Loss: 32.20132827758789\n",
      "Epoch: 12, Loss: 31.93497657775879\n",
      "Epoch: 13, Loss: 31.454566955566406\n",
      "Epoch: 14, Loss: 31.023502349853516\n",
      "Epoch: 15, Loss: 30.607654571533203\n",
      "Epoch: 16, Loss: 30.193368911743164\n",
      "Epoch: 17, Loss: 29.816753387451172\n",
      "Epoch: 18, Loss: 29.486560821533203\n",
      "Epoch: 19, Loss: 28.978042602539062\n",
      "Epoch: 20, Loss: 28.741657257080078\n",
      "Epoch: 21, Loss: 28.21759033203125\n",
      "Epoch: 22, Loss: 27.857959747314453\n",
      "Epoch: 23, Loss: 27.46573829650879\n",
      "Epoch: 24, Loss: 27.04524040222168\n",
      "Epoch: 25, Loss: 26.657852172851562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m loss.backward()\n\u001b[32m     11\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "\toptimizer.zero_grad()\n",
    "\toutput = transformer(src_data, tgt_data)\n",
    "\tloss = criterion(output[:, 1:, :].contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\tprint(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
