_target_: models.chemformer.Chemformer
# d_model: 768
d_model: 512
# n_heads: 12
n_heads: 8
n_layers: 6
# d_ff: 3072 # typically d_model * 4
d_ff: 2048 # typically d_model * 4
norm_layer: ${py:torch.nn.LayerNorm}
activation: "gelu"
