defaults:
  - dataset: uspto
  - tokenizer: hf
  - model: chemformer
  - module: bart
  - _self_

seed: 24
train_split: 0.9

batch_size: 2
num_workers: 4
# ckpt_path: "train_checkpoints-server/best-checkpoint-filtered-chembl-v1.ckpt"
pth_path: ""

callbacks:
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: "val_loss"
    patience: 5
    verbose: true
    min_delta: 0.001
    mode: "min"

  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: "val_loss"
    dirpath: "train_checkpoints"
    filename: "best-checkpoint-pretrain-zinc-neochem"
    save_top_k: 1
    mode: "min"

  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: "train_checkpoints"
    filename: "latest-checkpoint-pretrain-zinc-neochem"
    save_last: true

  - _target_: lightning.pytorch.callbacks.Timer
    duration: "00:20:00:00"

loggers:
  - _target_: lightning.pytorch.loggers.TensorBoardLogger
    save_dir: "lightning_logs"
    name: "pretrain_zinc_neochem"
  - _target_: lightning.pytorch.loggers.CSVLogger
    save_dir: "logs"
    name: "pretrain_zinc_neochem"

# pretrained_state_dict: "chemformer_small_2.pth"

trainer:
  # max_epochs: 50
  # max_steps: 1000000
  # val_check_interval: 10000
  max_steps: 10000
  val_check_interval: 5000
  precision: "16-mixed"
  gradient_clip_val: 1.0
  # log_every_n_steps: 50
  # accumulate_grad_batches: 1
  # limit_train_batches: 0.0001
  limit_val_batches: 0.1
  # limit_test_batches: 0.0001

task: fit
task_type: pretrain
